{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "brief-observation",
   "metadata": {},
   "source": [
    "# COMP40020: Human Language Technologies\n",
    "## Assignment 1\n",
    "\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "For this Assignment you will be investigating the language styles used online by processing and analysing data from Reddit. You should submit a report online via Brightspace by **11:59pm Wednesday 10th March 2021**. There are no extensions to this deadline without formal extenuating circumstances. Please contact us as soon as possible should you require an extension. All assignments will undergo a plagiarism check as standard.\n",
    "\n",
    "\n",
    "### Task\n",
    "\n",
    "For this assignment you will build a corpus (or multiple corpora) of internet language data from one or more subreddits, decide which preproccessing steps should be carried out, then perform some analysis on it to investigate a particular aspect of language usage. You can use any of the tools presented in previous workshops or come up with your own methods of analysis. \n",
    "\n",
    "Some examples of things you might want to investigate:\n",
    "\n",
    "* can you infer the subject of a subreddit from its most common words?\n",
    "* does the style of language differ if you compare different subjects?\n",
    "* are there subreddits that are very similar in their language?\n",
    "* do common n-grams capture popular jokes or memes within the subreddit?\n",
    "* are there texts which are more lexically diverse than others?\n",
    "* can you spot trends or popular topics over time?\n",
    "* are particular texts more likely to use certain emojis over others?\n",
    "\n",
    "This is not an exhaustive list nor do we expect you to answer all of these questions. Pick a research focus that interests you - a small specific question is more effective than a vague general investigation. If you have any questions or issues feel free to email me (emma.l.oneill@ucdconnect.ie). \n",
    "\n",
    "Please also be aware that we are working with real-world data from the internet. Use your own judgement in the case of potentially inappropriate or sensitive content. If your investigation deals with such content it should be handled professionally and respectfully. \n",
    "\n",
    "To get you started I have supplied steps and code to scrape data from Reddit. To do this you will need to sign up for a Reddit account. If you are unable to create an account or do not wish to do so you can collect data elsewhere. For example you could work with different books and authors by downloading texts from Project Gutenberg (https://www.gutenberg.org/).\n",
    "\n",
    "\n",
    "### Format\n",
    "\n",
    "The assignment should be submitted as a **PDF file**. It should take the form of a short scientific report with the following sections;\n",
    "\n",
    "* Research Question - what is the focus of your investigation\n",
    "* Methods - a description of the data you collected, the preprocessing steps you carried out (and why), and the analysis you performed\n",
    "* Results - the results of your analysis\n",
    "* Discussion - an interpretation of your results; what they mean, conclusions you can draw, any potential issues or areas of future work\n",
    "\n",
    "The report should be around 2000 words (absolute maximum 3000). Please include a word count at the end of your assignment.\n",
    "\n",
    "**You should also upload the .ipynb notebook file and any additional files needed to run your code (like your corpus) as a separate .zip file** These will **not** be graded but may be examined to verify your results or check for plagiarism.\n",
    "\n",
    "Any code that was supplied here or in the Workshops does not need to be included in your report. If you have written any additional code of your own please include relevant snippets where appropriate. \n",
    "\n",
    "Any referenced material should be properly cited and it should be clear where work is not your own.\n",
    "\n",
    "Again, submission should be a **PDF** of your report and a **separate .zip** of your notebook and corpus. \n",
    "\n",
    "### Grading\n",
    "\n",
    "This Assignment accounts for 20% of the overall grade for this module. The assignment will be letter graded according to the UCD grading scheme for Stage 4 modules.\n",
    "\n",
    "(see: https://www.ucd.ie/registry/t4media/UCD%20Module%20Grade%20Descriptors.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deluxe-italian",
   "metadata": {},
   "source": [
    "## Accessing Reddit\n",
    "\n",
    "To access reddit data you will need to follow these steps:\n",
    "\n",
    "1. Sign in to reddit (feel free to create a new account for this Assignment and delete it afterwards)\n",
    "\n",
    "\n",
    "2. Go to https://www.reddit.com/prefs/apps and click \"are you a developer? create an app...\" \n",
    "\n",
    "\n",
    "3. Enter a name (combine a random word and some numbers - if it's too short you might have problems later), select \"script\" and for the redirect uri enter http://localhost:8080\n",
    "\n",
    "\n",
    "4. Click \"create app\"\n",
    "\n",
    "\n",
    "5. You should now see the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "preceding-classroom",
   "metadata": {},
   "source": [
    "<img src=\"reddit.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "meaning-ocean",
   "metadata": {},
   "source": [
    "For scraping our reddit data we'll be using PRAW. In a terminal (Mac: Terminal, Windows: cmd) type:\n",
    "\n",
    "<code>pip install praw</code>\n",
    "\n",
    "Now we're ready to start.\n",
    "\n",
    "## Building a corpus\n",
    "\n",
    "We will now create a .txt file containing a number of posts for a particular subreddit. Once this code has been run you should have a file (SUBREDDIT_NAME.txt) that you can work with using the tools presented in previous workshops.\n",
    "\n",
    "Using the image above for reference, fill in your own values in between the \" \" marks for the following:\n",
    "\n",
    "(For examples of subreddits you could look at see https://blog.oneupapp.io/biggest-subreddits/ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "isolated-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = \"\" # your client id\n",
    "cs = \"\" #your client secret\n",
    "ua = \"\" #your user agent name\n",
    "sub = \"\" #the name of the subreddit (not including the 'r/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animated-uniform",
   "metadata": {},
   "source": [
    "Then run the following cell to create a corpus containing post titles, content and top level comments:\n",
    "\n",
    "Note that you can change how posts are sorted, e.g. \"hot\", \"new\", \"rising\", \"top\" and how many posts you collect - your choice will be dependent on your research question and the subreddit you choose. Depending on how many posts you extract and how many comments there are this could take a few minutes. You might also choose not to extract titles or comments - feel free to remove the corresponding lines of code in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "innovative-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from praw.models import MoreComments\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=ci,\n",
    "    client_secret=cs,\n",
    "    user_agent=ua\n",
    ")\n",
    "\n",
    "with open(sub+\".txt\", \"w\") as f:\n",
    "    \n",
    "    #on the following line you can change top to any of the previously mentioned ways of sorting \n",
    "    #and the limit to however many posts you would like to extract (here we extract just 10).\n",
    "    for post in reddit.subreddit(sub).top(limit=10): \n",
    "        \n",
    "        #this line collects the post titles\n",
    "        f.write(post.title+\"\\n\")\n",
    "        \n",
    "        #this line collects the post content\n",
    "        f.write(post.selftext+\"\\n\")\n",
    "        \n",
    "        #this section collects the comments\n",
    "        for comment in post.comments:\n",
    "            if isinstance(comment, MoreComments):\n",
    "                continue\n",
    "            f.write(comment.body+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romance-poison",
   "metadata": {},
   "source": [
    "As a final note, you are not restricted to this methodology, file format, or the analysis tools from Workshops. You are very welcome to try out your own ideas in terms of coding, data structures, and/or linguistic theories. If you have an idea you want to implement but aren't sure how to go about it let me know - you are not being graded on your code but rather your ability to apply techniques, generate results, and discuss your interpretations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blank-minimum",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
