{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "altered-silly",
   "metadata": {},
   "source": [
    "Thomas Igoe // 17372013\n",
    "HLT Assignement 1 code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smaller-bubble",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "latin-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "from praw.models import MoreComments\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "import emoji\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-prisoner",
   "metadata": {},
   "source": [
    "Remove emojis no longer used but both were solutions to allowing me to print to .txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "celtic-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all emojis from string\n",
    "def remove_emojis(text):\n",
    "    return text.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "\n",
    "# Convert all emojis in a string to :string_representation:\n",
    "def decode_emojis(text):\n",
    "    return emoji.demojize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sunrise-treatment",
   "metadata": {},
   "source": [
    "This displays all comments for a given comment tree (it does so for all comments recursively, and indents them accordingly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "international-somerset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recursively display all comments\n",
    "def display_comments(comments, indent=\" \"):\n",
    "    for reply in comments:\n",
    "        if not hasattr(reply, 'body'):\n",
    "            return\n",
    "\n",
    "        tokens = nltk.word_tokenize(reply.body)\n",
    "\n",
    "        print(indent + \" \" + reply.body + \"(length: \" + str(len(tokens)) + \")\")\n",
    "        display_comments(reply.replies, indent + \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "further-neighbor",
   "metadata": {},
   "source": [
    "Lexical diversity calculation w/ handling for empty files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "immune-pavilion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexical complexity of a list of tokens\n",
    "def lexical_complexity(tokens):\n",
    "    try:\n",
    "        return len(set(tokens)) / len(tokens)\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "previous-rebate",
   "metadata": {},
   "source": [
    "Gets a list of lexical diversitys for each comment in a comment tree (unused)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "twenty-champagne",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lex diversity of a set of comments\n",
    "def individual_lex_complexity(comments):\n",
    "    diversity_scores = []\n",
    "\n",
    "    for reply in comments:\n",
    "        if not hasattr(reply, 'body'):\n",
    "            return diversity_scores\n",
    "\n",
    "        tokens = nltk.word_tokenize(reply.body)\n",
    "        diversity_scores.append(lexical_complexity(tokens))\n",
    "        print(tokens)\n",
    "        print(lexical_complexity(tokens))\n",
    "        print()\n",
    "\n",
    "        temp_diversity_list = average_individual_lex_complexity(reply.replies)\n",
    "\n",
    "        diversity_scores.extend(temp_diversity_list)\n",
    "\n",
    "    return diversity_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "otherwise-thickness",
   "metadata": {},
   "source": [
    "Builds a corpus of comments for a subreddit\n",
    "\n",
    "when basic == true, only write to the all.txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "utility-hazard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Builds a corpus of top level comments for a subreddit\n",
    "# When basic is true only write all.txt\n",
    "def build_subreddit_corpus(subreddit, reddit, basic=False):\n",
    "    post_limit = 100\n",
    "\n",
    "    if not os.path.exists(subreddit):\n",
    "        os.makedirs(subreddit)\n",
    "\n",
    "    negative = open(subreddit + \"/negative.txt\", \"w\", encoding='utf8')\n",
    "    one = open(subreddit + \"/gt1.txt\", \"w\", encoding='utf8')\n",
    "    ten = open(subreddit + \"/gt10.txt\", \"w\", encoding='utf8')\n",
    "    hundred = open(subreddit + \"/gt100.txt\", \"w\", encoding='utf8')\n",
    "    thousand = open(subreddit + \"/gt1000.txt\", \"w\", encoding='utf8')\n",
    "    tenthousand = open(subreddit + \"/gt10000.txt\", \"w\", encoding='utf8')\n",
    "    overall = open(subreddit + \"/all.txt\", \"w\", encoding='utf8')\n",
    "\n",
    "    # on the following line you can change top to any of the previously mentioned ways of sorting\n",
    "    # and the limit to however many posts you would like to extract (here we extract just 10).\n",
    "\n",
    "    count = 0\n",
    "    for post in reddit.subreddit(subreddit).top(limit=post_limit):\n",
    "        count = count + 1\n",
    "        print(\"Working on post \" + str(count) + \"/\" + str(post_limit) + \" in \" + subreddit)\n",
    "\n",
    "        # this section collects the comments\n",
    "        for comment in post.comments:\n",
    "\n",
    "            if isinstance(comment, MoreComments):\n",
    "                continue\n",
    "\n",
    "            if comment.body == \"[removed]\" or comment.body == \"[deleted]\":\n",
    "                continue\n",
    "\n",
    "            formatted_comment = decode_emojis(comment.body) + \"\\n\"\n",
    "            try:\n",
    "                overall.write(formatted_comment)\n",
    "                if not basic:\n",
    "                    if comment.score >= 10000:\n",
    "                        tenthousand.write(formatted_comment)\n",
    "                    elif comment.score >= 1000:\n",
    "                        thousand.write(formatted_comment)\n",
    "                    elif comment.score >= 100:\n",
    "                        hundred.write(formatted_comment)\n",
    "                    elif comment.score >= 10:\n",
    "                        ten.write(formatted_comment)\n",
    "                    elif comment.score >= 1:\n",
    "                        one.write(formatted_comment)\n",
    "                    else:\n",
    "                        negative.write(formatted_comment)\n",
    "            except UnicodeEncodeError:\n",
    "                print(\"String contains character causing errors:\")\n",
    "                print(formatted_comment)\n",
    "\n",
    "    negative.close()\n",
    "    one.close()\n",
    "    ten.close()\n",
    "    hundred.close()\n",
    "    thousand.close()\n",
    "    tenthousand.close()\n",
    "    overall.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-tenant",
   "metadata": {},
   "source": [
    "Graphs the score distribution of all comments in a subreddit\n",
    "\n",
    "Not polished, and takes WAY too long (dont use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "significant-chicago",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the score distribution of comments in \"subreddit\"\n",
    "def score_distribution(subreddit, reddit):\n",
    "    post_limit = 200\n",
    "\n",
    "    count = 0\n",
    "    scores = [0] * 1000\n",
    "    for post in reddit.subreddit(subreddit).top(limit=post_limit):\n",
    "        count = count + 1\n",
    "        print(\"Working on post \" + str(count) + \"/\" + str(post_limit) + \" in \" + subreddit)\n",
    "\n",
    "        # this section collects the comments\n",
    "        for comment in post.comments:\n",
    "\n",
    "            if isinstance(comment, MoreComments):\n",
    "                continue\n",
    "\n",
    "            a = 100\n",
    "\n",
    "            scores[int(comment.score / a)] = scores[int(comment.score / a)] + 1\n",
    "\n",
    "    x_axis = range(len(scores))\n",
    "    y_axis = scores\n",
    "    # print(x_axis)\n",
    "    # print(y_axis)\n",
    "    plt.bar(x_axis, y_axis, color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extensive-trailer",
   "metadata": {},
   "source": [
    "Summarizes the lexical diversity of a subreddit\n",
    "\n",
    "when complex_tokenization == true, use custom_tokenize_plus which removes stopwords and stems words. Takes a lot of time to do it this way so disabled for testing.\n",
    "\n",
    "when complex_output == true, show lex diversity of all files, otherwise just show all.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fixed-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarises the lexical diversity of a sub at various scores\n",
    "def lexical_diversity_summary(sub_directory, complex_tokenization=False, complex_output=False):\n",
    "    if not os.path.exists(sub_directory):\n",
    "        print(\"Subreddit directory \" + sub_directory + \" does not exist\")\n",
    "        return\n",
    "\n",
    "    negative = open(sub_directory + \"/negative.txt\", \"r\", encoding=\"utf8\")\n",
    "    one = open(sub_directory + \"/gt1.txt\", \"r\", encoding=\"utf8\")\n",
    "    ten = open(sub_directory + \"/gt10.txt\", \"r\", encoding=\"utf8\")\n",
    "    hundred = open(sub_directory + \"/gt100.txt\", \"r\", encoding=\"utf8\")\n",
    "    thousand = open(sub_directory + \"/gt1000.txt\", \"r\", encoding=\"utf8\")\n",
    "    tenthousand = open(sub_directory + \"/gt10000.txt\", \"r\", encoding=\"utf8\")\n",
    "    overall = open(sub_directory + \"/all.txt\", \"r\", encoding=\"utf8\")\n",
    "\n",
    "    \n",
    "    tenthousand_tokens = custom_tokenize(tenthousand.read())\n",
    "    sample_size = len(tenthousand_tokens)\n",
    "    \n",
    "    if sample_size == 0:\n",
    "        sample_size = -1\n",
    "    \n",
    "\n",
    "    if complex_tokenization:\n",
    "        print(\"Lexical diversity of all comments:\\t\" +\n",
    "              str(lexical_complexity(custom_tokenize_plus(overall.read())[:sample_size])))\n",
    "        if complex_output:\n",
    "            print(\"Lexical diversity of posts w score >10,000:\\t\" +\n",
    "                  str(lexical_complexity(tenthousand_tokens[:sample_size])))\n",
    "            print(\"Lexical diversity of posts w score >1,000:\\t\" +\n",
    "                  str(lexical_complexity(custom_tokenize_plus(thousand.read())[:sample_size])))\n",
    "            print(\"Lexical diversity of posts w score >100:\\t\" +\n",
    "                  str(lexical_complexity(custom_tokenize_plus(hundred.read())[:sample_size])))\n",
    "            print(\"Lexical diversity of posts w score >10:\\t\\t\" +\n",
    "                  str(lexical_complexity(custom_tokenize_plus(ten.read())[:sample_size])))\n",
    "            print(\"Lexical diversity of posts w score >1:\\t\\t\" +\n",
    "                  str(lexical_complexity(custom_tokenize_plus(one.read())[:sample_size])))\n",
    "            print(\"Lexical diversity of posts w negative score:\\t\\t\" +\n",
    "                  str(lexical_complexity(custom_tokenize_plus(negative.read())[:sample_size])))\n",
    "    else:\n",
    "        print(\"Lexical diversity of all comments:\\t\" +\n",
    "              str(lexical_complexity(custom_tokenize(overall.read())[:sample_size])))\n",
    "        if complex_output:\n",
    "            print(\"Lexical diversity of posts w score >10,000:\\t\" +\n",
    "                  str(lexical_complexity(tenthousand_tokens[:sample_size])))\n",
    "            print(\"Lexical diversity of posts w score >1,000:\\t\" +\n",
    "                  str(lexical_complexity(custom_tokenize(thousand.read())[:sample_size])))\n",
    "            print(\"Lexical diversity of posts w score >100:\\t\" +\n",
    "                  str(lexical_complexity(custom_tokenize(hundred.read())[:sample_size])))\n",
    "            print(\"Lexical diversity of posts w score >10:\\t\\t\" +\n",
    "                  str(lexical_complexity(custom_tokenize(ten.read())[:sample_size])))\n",
    "            print(\"Lexical diversity of posts w score >1:\\t\\t\" +\n",
    "                  str(lexical_complexity(custom_tokenize(one.read())[:sample_size])))\n",
    "            print(\"Lexical diversity of posts w negative score:\\t\\t\" +\n",
    "                  str(lexical_complexity(custom_tokenize(negative.read())[:sample_size])))\n",
    "\n",
    "    negative.close()\n",
    "    one.close()\n",
    "    ten.close()\n",
    "    hundred.close()\n",
    "    thousand.close()\n",
    "    tenthousand.close()\n",
    "    overall.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mysterious-federal",
   "metadata": {},
   "source": [
    "Used to tokenize and do basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "subject-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the words while doing preprocessing.\n",
    "def custom_tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    lowercase = [t.lower() for t in tokens]\n",
    "    no_punct = [t for t in lowercase if t.isalnum()]\n",
    "    # no_stop = [t for t in no_punct if t not in stopwords.words('english')]\n",
    "    return no_punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "diagnostic-sterling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and remove stopwords\n",
    "def custom_tokenize_plus(text):\n",
    "    tokens = custom_tokenize(text)\n",
    "    no_stop = [t for t in tokens if t not in stopwords.words('english')]\n",
    "    porter = PorterStemmer()\n",
    "    return [porter.stem(w) for w in no_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dried-engineering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the Q most common ngrams of a text\n",
    "def common_ngrams(text, n, q):\n",
    "    tokens = custom_tokenize(text)\n",
    "    gram_fd = nltk.FreqDist(nltk.ngrams(tokens, n))\n",
    "    return gram_fd.most_common(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dramatic-ancient",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the common n-grams of a subreddit at different scores\n",
    "def common_ngram_summary(sub_directory):\n",
    "    if not os.path.exists(sub_directory):\n",
    "        print(\"Subreddit directory \" + sub_directory + \" does not exist\")\n",
    "        return\n",
    "\n",
    "    ngram_min = 1\n",
    "    ngram_max = 4\n",
    "\n",
    "    ngram_quantity = 20\n",
    "\n",
    "    for x in range(ngram_min, ngram_max + 1):\n",
    "        negative = open(sub_directory + \"/negative.txt\", \"r\", encoding=\"utf8\")\n",
    "        one = open(sub_directory + \"/gt1.txt\", \"r\", encoding=\"utf8\")\n",
    "        ten = open(sub_directory + \"/gt10.txt\", \"r\", encoding=\"utf8\")\n",
    "        hundred = open(sub_directory + \"/gt100.txt\", \"r\", encoding=\"utf8\")\n",
    "        thousand = open(sub_directory + \"/gt1000.txt\", \"r\", encoding=\"utf8\")\n",
    "        tenthousand = open(sub_directory + \"/gt10000.txt\", \"r\", encoding=\"utf8\")\n",
    "        overall = open(sub_directory + \"/all.txt\", \"r\", encoding=\"utf8\")\n",
    "\n",
    "        print(\"Most common \" + str(x) + \"-grams of all posts:\\t\" +\n",
    "              str(common_ngrams(overall.read(), x, ngram_quantity)))\n",
    "        print(\"Most common \" + str(x) + \"-grams of posts w score >10,000:\\t\" +\n",
    "              str(common_ngrams(tenthousand.read(), x, ngram_quantity)))\n",
    "        print(\"Most common \" + str(x) + \"-grams of posts w score >1,000:\\t\" +\n",
    "              str(common_ngrams(thousand.read(), x, ngram_quantity)))\n",
    "        print(\"Most common \" + str(x) + \"-grams of posts w score >100:\\t\" +\n",
    "              str(common_ngrams(hundred.read(), x, ngram_quantity)))\n",
    "        print(\"Most common \" + str(x) + \"-grams of posts w score >10:\\t\" +\n",
    "              str(common_ngrams(ten.read(), x, ngram_quantity)))\n",
    "        print(\"Most common \" + str(x) + \"-grams of posts w score >1:\\t\" +\n",
    "              str(common_ngrams(one.read(), x, ngram_quantity)))\n",
    "        print()\n",
    "\n",
    "    negative.close()\n",
    "    one.close()\n",
    "    ten.close()\n",
    "    hundred.close()\n",
    "    thousand.close()\n",
    "    tenthousand.close()\n",
    "    overall.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executive-career",
   "metadata": {},
   "source": [
    "Env setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "vocational-cycle",
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = \"f2CDXIEJMw9amg\"  # your client id\n",
    "cs = \"dS2I4meWoXMgQ-AtZObOO8C5WM_smw\"  # your client secret\n",
    "ua = \"HLTUCDScript\"  # your user agent name\n",
    "sub = \"todayilearned\"  # the name of the subreddit (not including the 'r/')\n",
    "\n",
    "# nltk.download('punkt')\n",
    "\n",
    "reddit = praw.Reddit(\n",
    "    client_id=ci,\n",
    "    client_secret=cs,\n",
    "    user_agent=ua\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "creative-aviation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lexical diversity of all comments:\t0.2742948094438058\n",
      "Lexical diversity of posts w score >10,000:\t0.30771951543018833\n",
      "Lexical diversity of posts w score >1,000:\t0.2982580245821912\n",
      "Lexical diversity of posts w score >100:\t0.2918914139181183\n",
      "Lexical diversity of posts w score >10:\t\t0.29392519232469716\n",
      "Lexical diversity of posts w score >1:\t\t0.2776549650720665\n"
     ]
    }
   ],
   "source": [
    "lexical_diversity_summary(\"todayilearned\", complex_tokenization=True, complex_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cordless-governor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "todayilearned\n",
      "Lexical diversity of all comments:\t0.1810831223066329\n",
      "Lexical diversity of posts w score >10,000:\t0.20445155271224844\n",
      "Lexical diversity of posts w score >1,000:\t0.1988982184903816\n",
      "Lexical diversity of posts w score >100:\t0.1919232307077169\n",
      "Lexical diversity of posts w score >10:\t\t0.1938335776800391\n",
      "Lexical diversity of posts w score >1:\t\t0.18050557554755875\n",
      "\n",
      "funny\n",
      "Lexical diversity of all comments:\t0.2367074210975263\n",
      "Lexical diversity of posts w score >10,000:\t0.27921524026158656\n",
      "Lexical diversity of posts w score >1,000:\t0.27779357406880867\n",
      "Lexical diversity of posts w score >100:\t0.26201307932897355\n",
      "Lexical diversity of posts w score >10:\t\t0.26158657947114017\n",
      "Lexical diversity of posts w score >1:\t\t0.2477964174011942\n",
      "\n",
      "science\n",
      "Lexical diversity of all comments:\t0.2362179487179487\n",
      "Lexical diversity of posts w score >10,000:\t0.2721153846153846\n",
      "Lexical diversity of posts w score >1,000:\t0.2955128205128205\n",
      "Lexical diversity of posts w score >100:\t0.29599358974358975\n",
      "Lexical diversity of posts w score >10:\t\t0.28669871794871793\n",
      "Lexical diversity of posts w score >1:\t\t0.2693910256410256\n",
      "\n",
      "gaming\n",
      "Lexical diversity of all comments:\t0.2067319169252805\n",
      "Lexical diversity of posts w score >10,000:\t0.2554308904273096\n",
      "Lexical diversity of posts w score >1,000:\t0.2557889711148245\n",
      "Lexical diversity of posts w score >100:\t0.24600143232275007\n",
      "Lexical diversity of posts w score >10:\t\t0.2431367868226307\n",
      "Lexical diversity of posts w score >1:\t\t0.22511339221771307\n",
      "\n",
      "leagueoflegends\n",
      "Lexical diversity of all comments:\t0.36019736842105265\n",
      "Lexical diversity of posts w score >10,000:\t0.40789473684210525\n",
      "Lexical diversity of posts w score >1,000:\t0.4194078947368421\n",
      "Lexical diversity of posts w score >100:\t0.421875\n",
      "Lexical diversity of posts w score >10:\t\t0.42680921052631576\n",
      "Lexical diversity of posts w score >1:\t\t0.3560855263157895\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subreddits = [\n",
    "    \"todayilearned\",\n",
    "    \"funny\",\n",
    "    \"science\",\n",
    "    \"gaming\",\n",
    "    \"leagueoflegends\"\n",
    "]\n",
    "\n",
    "for r in subreddits:\n",
    "    print(r)\n",
    "    lexical_diversity_summary(r, complex_tokenization=False, complex_output=True)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "proof-painting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common 1-grams of all posts:\t[(('the',), 24279), (('a',), 14563), (('to',), 14470), (('and',), 12947), (('i',), 12666), (('of',), 10752), (('in',), 8355), (('that',), 8042), (('it',), 7888), (('was',), 6880), (('is',), 6056), (('he',), 5422), (('for',), 5100), (('this',), 4989), (('you',), 4509), (('they',), 4221), (('on',), 3725), (('my',), 3604), (('with',), 3347), (('but',), 2900), (('have',), 2880), (('as',), 2725), (('his',), 2709), (('be',), 2643), (('not',), 2527), (('so',), 2495), (('at',), 2356), (('like',), 2344), (('are',), 2217), (('just',), 2203), (('we',), 2180), (('do',), 2137), (('me',), 2080), (('if',), 2076), (('had',), 2056), (('people',), 2043), (('all',), 2027), (('an',), 1961), (('when',), 1959), (('one',), 1949)]\n",
      "Most common 1-grams of posts w score >10,000:\t[(('the',), 1089), (('to',), 643), (('a',), 571), (('and',), 555), (('i',), 452), (('of',), 427), (('in',), 381), (('that',), 336), (('was',), 311), (('he',), 285), (('it',), 266), (('for',), 234), (('you',), 223), (('his',), 193), (('they',), 180), (('is',), 168), (('this',), 162), (('with',), 154), (('on',), 150), (('have',), 131), (('had',), 117), (('at',), 115), (('but',), 113), (('my',), 111), (('be',), 107), (('so',), 103), (('do',), 99), (('as',), 98), (('we',), 96), (('him',), 91), (('if',), 90), (('me',), 88), (('from',), 87), (('would',), 85), (('one',), 85), (('by',), 82), (('like',), 81), (('about',), 80), (('an',), 79), (('were',), 79)]\n",
      "Most common 1-grams of posts w score >1,000:\t[(('the',), 2755), (('to',), 1643), (('a',), 1628), (('and',), 1404), (('i',), 1269), (('of',), 1257), (('in',), 1009), (('that',), 857), (('it',), 815), (('was',), 768), (('he',), 682), (('is',), 614), (('for',), 558), (('they',), 494), (('you',), 490), (('this',), 427), (('on',), 409), (('my',), 399), (('with',), 380), (('his',), 344), (('as',), 315), (('have',), 300), (('but',), 299), (('not',), 275), (('at',), 267), (('so',), 264), (('be',), 245), (('one',), 225), (('if',), 224), (('had',), 221), (('an',), 220), (('when',), 217), (('me',), 214), (('like',), 212), (('people',), 210), (('just',), 209), (('are',), 207), (('there',), 202), (('we',), 197), (('do',), 196)]\n",
      "Most common 1-grams of posts w score >100:\t[(('the',), 4527), (('to',), 2563), (('a',), 2526), (('and',), 2306), (('i',), 1951), (('of',), 1947), (('in',), 1463), (('it',), 1398), (('that',), 1280), (('was',), 1212), (('is',), 1016), (('for',), 890), (('he',), 872), (('you',), 819), (('this',), 762), (('they',), 734), (('on',), 648), (('with',), 599), (('my',), 521), (('but',), 491), (('as',), 473), (('have',), 466), (('be',), 462), (('his',), 451), (('not',), 435), (('at',), 410), (('are',), 399), (('so',), 394), (('like',), 389), (('if',), 371), (('just',), 368), (('we',), 367), (('would',), 366), (('do',), 364), (('people',), 362), (('an',), 361), (('one',), 355), (('had',), 353), (('about',), 345), (('or',), 337)]\n",
      "Most common 1-grams of posts w score >10:\t[(('the',), 6589), (('a',), 4104), (('to',), 3999), (('and',), 3725), (('i',), 3581), (('of',), 3002), (('in',), 2237), (('it',), 2219), (('that',), 2155), (('was',), 1996), (('is',), 1710), (('for',), 1469), (('he',), 1377), (('this',), 1362), (('they',), 1238), (('you',), 1167), (('my',), 1051), (('on',), 1041), (('with',), 954), (('but',), 865), (('as',), 811), (('have',), 769), (('be',), 735), (('not',), 730), (('his',), 693), (('like',), 678), (('so',), 678), (('at',), 657), (('just',), 645), (('are',), 624), (('do',), 620), (('had',), 619), (('we',), 619), (('all',), 595), (('me',), 593), (('people',), 593), (('about',), 563), (('if',), 557), (('when',), 539), (('an',), 537)]\n",
      "Most common 1-grams of posts w score >1:\t[(('the',), 9317), (('a',), 5732), (('to',), 5621), (('i',), 5412), (('and',), 4956), (('of',), 4119), (('that',), 3414), (('in',), 3263), (('it',), 3190), (('was',), 2592), (('is',), 2548), (('this',), 2276), (('he',), 2204), (('for',), 1949), (('you',), 1810), (('they',), 1574), (('my',), 1522), (('on',), 1476), (('with',), 1260), (('have',), 1212), (('but',), 1132), (('be',), 1092), (('so',), 1056), (('as',), 1028), (('his',), 1028), (('not',), 1012), (('like',), 984), (('just',), 915), (('are',), 909), (('at',), 907), (('we',), 901), (('me',), 886), (('all',), 864), (('do',), 858), (('if',), 834), (('people',), 829), (('when',), 790), (('would',), 775), (('an',), 763), (('from',), 758)]\n",
      "\n",
      "Most common 2-grams of all posts:\t[(('of', 'the'), 2230), (('in', 'the'), 2186), (('to', 'the'), 1086), (('it', 'was'), 1008), (('i', 'was'), 936), (('to', 'be'), 933), (('this', 'is'), 877), (('he', 'was'), 838), (('on', 'the'), 823), (('was', 'a'), 754), (('for', 'the'), 733), (('one', 'of'), 708), (('is', 'a'), 699), (('and', 'the'), 677), (('and', 'i'), 677), (('in', 'a'), 640), (('as', 'a'), 588), (('at', 'the'), 579), (('if', 'you'), 533), (('for', 'a'), 487), (('a', 'lot'), 486), (('the', 'same'), 480), (('of', 'a'), 446), (('with', 'the'), 445), (('is', 'the'), 435), (('out', 'of'), 427), (('that', 'the'), 427), (('it', 's'), 426), (('from', 'the'), 422), (('to', 'get'), 416), (('when', 'i'), 413), (('i', 'have'), 400), (('all', 'the'), 399), (('but', 'i'), 399), (('to', 'do'), 398), (('they', 'were'), 390), (('with', 'a'), 372), (('it', 'is'), 366), (('have', 'to'), 364), (('i', 'do'), 359)]\n",
      "Most common 2-grams of posts w score >10,000:\t[(('in', 'the'), 100), (('of', 'the'), 92), (('to', 'the'), 50), (('it', 'was'), 48), (('to', 'be'), 45), (('for', 'the'), 41), (('he', 'was'), 40), (('was', 'a'), 34), (('at', 'the'), 34), (('this', 'is'), 32), (('and', 'the'), 31), (('one', 'of'), 31), (('and', 'i'), 29), (('in', 'a'), 28), (('of', 'his'), 27), (('for', 'a'), 26), (('the', 'same'), 25), (('and', 'he'), 24), (('on', 'the'), 23), (('i', 'was'), 22), (('from', 'the'), 22), (('with', 'the'), 21), (('if', 'you'), 21), (('to', 'do'), 21), (('that', 'the'), 20), (('is', 'a'), 19), (('the', 'time'), 19), (('had', 'a'), 18), (('that', 'i'), 18), (('that', 'he'), 18), (('i', 'do'), 17), (('would', 'be'), 17), (('would', 'have'), 17), (('and', 'it'), 16), (('a', 'lot'), 16), (('the', 'first'), 16), (('out', 'of'), 16), (('to', 'have'), 16), (('as', 'a'), 16), (('of', 'a'), 16)]\n",
      "Most common 2-grams of posts w score >1,000:\t[(('of', 'the'), 274), (('in', 'the'), 264), (('to', 'the'), 134), (('in', 'a'), 104), (('he', 'was'), 102), (('to', 'be'), 95), (('it', 'was'), 94), (('one', 'of'), 89), (('on', 'the'), 88), (('for', 'the'), 86), (('i', 'was'), 86), (('as', 'a'), 78), (('is', 'a'), 77), (('and', 'the'), 75), (('this', 'is'), 70), (('was', 'a'), 69), (('at', 'the'), 68), (('and', 'i'), 66), (('that', 'the'), 63), (('if', 'you'), 62), (('a', 'lot'), 61), (('of', 'a'), 60), (('i', 'have'), 52), (('to', 'get'), 52), (('for', 'a'), 51), (('out', 'of'), 50), (('but', 'i'), 50), (('that', 'he'), 49), (('lot', 'of'), 46), (('is', 'the'), 46), (('they', 'were'), 45), (('with', 'the'), 43), (('i', 'think'), 43), (('it', 'is'), 42), (('with', 'a'), 42), (('from', 'the'), 42), (('have', 'to'), 41), (('the', 'same'), 40), (('all', 'the'), 40), (('so', 'i'), 40)]\n",
      "Most common 2-grams of posts w score >100:\t[(('of', 'the'), 435), (('in', 'the'), 398), (('to', 'the'), 205), (('on', 'the'), 177), (('it', 'was'), 174), (('to', 'be'), 155), (('and', 'the'), 139), (('he', 'was'), 138), (('i', 'was'), 138), (('for', 'the'), 136), (('one', 'of'), 131), (('this', 'is'), 127), (('is', 'a'), 125), (('was', 'a'), 115), (('at', 'the'), 111), (('and', 'i'), 105), (('in', 'a'), 103), (('as', 'a'), 100), (('out', 'of'), 99), (('the', 'same'), 94), (('if', 'you'), 90), (('with', 'the'), 85), (('that', 'the'), 83), (('from', 'the'), 81), (('3', '3'), 77), (('a', 'lot'), 75), (('for', 'a'), 75), (('of', 'a'), 75), (('is', 'the'), 74), (('to', 'do'), 69), (('had', 'a'), 68), (('they', 'were'), 68), (('to', 'get'), 67), (('would', 'be'), 67), (('with', 'a'), 66), (('i', 'have'), 64), (('was', 'the'), 64), (('it', 'is'), 63), (('to', 'a'), 63), (('a', 'few'), 60)]\n",
      "Most common 2-grams of posts w score >10:\t[(('of', 'the'), 583), (('in', 'the'), 582), (('i', 'was'), 320), (('it', 'was'), 309), (('to', 'the'), 291), (('to', 'be'), 262), (('on', 'the'), 232), (('was', 'a'), 230), (('this', 'is'), 219), (('he', 'was'), 211), (('for', 'the'), 207), (('and', 'the'), 200), (('and', 'i'), 198), (('is', 'a'), 185), (('one', 'of'), 184), (('as', 'a'), 178), (('in', 'a'), 160), (('at', 'the'), 155), (('if', 'you'), 151), (('with', 'the'), 142), (('a', 'lot'), 141), (('the', 'same'), 136), (('for', 'a'), 136), (('when', 'i'), 127), (('of', 'a'), 124), (('they', 'were'), 123), (('with', 'a'), 122), (('it', 's'), 121), (('to', 'get'), 120), (('but', 'i'), 115), (('to', 'do'), 114), (('that', 'the'), 112), (('out', 'of'), 111), (('i', 'do'), 111), (('i', 'think'), 110), (('all', 'the'), 109), (('i', 'had'), 109), (('i', 'have'), 109), (('going', 'to'), 107), (('had', 'a'), 105)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common 2-grams of posts w score >1:\t[(('of', 'the'), 845), (('in', 'the'), 841), (('this', 'is'), 429), (('to', 'the'), 406), (('it', 'was'), 383), (('to', 'be'), 376), (('i', 'was'), 369), (('he', 'was'), 347), (('was', 'a'), 306), (('on', 'the'), 303), (('is', 'a'), 293), (('and', 'i'), 279), (('one', 'of'), 273), (('for', 'the'), 262), (('in', 'a'), 245), (('and', 'the'), 232), (('as', 'a'), 216), (('at', 'the'), 212), (('if', 'you'), 209), (('it', 's'), 203), (('for', 'a'), 199), (('is', 'the'), 198), (('a', 'lot'), 193), (('all', 'the'), 190), (('the', 'same'), 185), (('when', 'i'), 178), (('from', 'the'), 175), (('of', 'a'), 171), (('but', 'i'), 167), (('i', 'have'), 164), (('have', 'a'), 161), (('to', 'get'), 160), (('to', 'do'), 158), (('with', 'the'), 154), (('have', 'to'), 154), (('it', 'is'), 152), (('i', 'do'), 152), (('out', 'of'), 151), (('that', 'the'), 147), (('going', 'to'), 145)]\n",
      "\n",
      "Most common 3-grams of all posts:\t[(('one', 'of', 'the'), 395), (('a', 'lot', 'of'), 327), (('when', 'i', 'was'), 174), (('at', 'the', 'time'), 131), (('it', 'was', 'a'), 130), (('reminds', 'me', 'of'), 107), (('the', 'rest', 'of'), 105), (('this', 'is', 'the'), 103), (('to', 'be', 'a'), 102), (('3', '3', '3'), 101), (('out', 'of', 'the'), 98), (('one', 'of', 'my'), 95), (('this', 'is', 'a'), 93), (('there', 'was', 'a'), 91), (('i', 'was', 'in'), 91), (('he', 'was', 'a'), 90), (('in', 'the', 'world'), 89), (('i', 'don', 't'), 89), (('in', 'the', 'us'), 84), (('i', 'used', 'to'), 83), (('this', 'is', 'why'), 81), (('i', 'was', 'a'), 80), (('there', 'is', 'a'), 78), (('is', 'one', 'of'), 78), (('was', 'one', 'of'), 77), (('you', 'have', 'to'), 75), (('some', 'of', 'the'), 74), (('a', 'bunch', 'of'), 72), (('a', 'few', 'years'), 72), (('i', 'feel', 'like'), 71), (('the', 'fact', 'that'), 70), (('i', 'have', 'a'), 70), (('would', 'have', 'been'), 68), (('be', 'able', 'to'), 68), (('i', 'went', 'to'), 67), (('part', 'of', 'the'), 66), (('one', 'of', 'those'), 65), (('i', 'had', 'a'), 64), (('i', 'want', 'to'), 64), (('most', 'of', 'the'), 63)]\n",
      "Most common 3-grams of posts w score >10,000:\t[(('one', 'of', 'the'), 22), (('a', 'lot', 'of'), 12), (('at', 'the', 'time'), 8), (('when', 'i', 'was'), 8), (('it', 'was', 'a'), 7), (('the', 'rest', 'of'), 6), (('he', 'was', 'a'), 6), (('thanks', 'for', 'the'), 6), (('for', 'the', 'gold'), 6), (('in', 'the', 'wall'), 6), (('this', 'is', 'a'), 6), (('of', 'the', 'most'), 6), (('and', 'it', 'was'), 5), (('thank', 'you', 'for'), 5), (('you', 'want', 'to'), 4), (('the', 'guy', 'who'), 4), (('most', 'of', 'the'), 4), (('i', 'do', 'think'), 4), (('all', 'the', 'time'), 4), (('i', 'didn', 't'), 4), (('of', 'my', 'life'), 4), (('up', 'to', 'the'), 4), (('the', 'front', 'row'), 4), (('i', 'was', 'a'), 4), (('if', 'you', 'are'), 4), (('i', 'do', 'know'), 4), (('that', 'why', 'i'), 4), (('to', 'do', 'it'), 4), (('it', 'was', 'the'), 4), (('went', 'to', 'the'), 4), (('was', 'going', 'to'), 4), (('did', 'the', 'same'), 4), (('a', 'few', 'years'), 4), (('i', 'used', 'to'), 4), (('i', 'can', 't'), 4), (('that', 'it', 'was'), 4), (('product', 'or', 'service'), 4), (('to', 'worry', 'about'), 3), (('that', 'was', 'a'), 3), (('on', 'to', 'the'), 3)]\n",
      "Most common 3-grams of posts w score >1,000:\t[(('one', 'of', 'the'), 53), (('a', 'lot', 'of'), 45), (('at', 'the', 'time'), 22), (('there', 'is', 'a'), 16), (('when', 'i', 'was'), 15), (('you', 'have', 'to'), 12), (('one', 'of', 'my'), 12), (('in', 'the', 'world'), 12), (('this', 'is', 'the'), 12), (('some', 'of', 'the'), 11), (('i', 'used', 'to'), 11), (('a', 'bunch', 'of'), 10), (('was', 'able', 'to'), 10), (('the', 'rest', 'of'), 10), (('a', 'few', 'years'), 10), (('out', 'of', 'the'), 10), (('would', 'have', 'been'), 9), (('all', 'of', 'the'), 9), (('part', 'of', 'the'), 9), (('was', 'one', 'of'), 9), (('reminds', 'me', 'of'), 9), (('of', 'the', 'world'), 8), (('to', 'be', 'a'), 8), (('he', 'was', 'a'), 8), (('that', 'he', 'was'), 8), (('i', 'was', 'in'), 8), (('was', 'in', 'the'), 8), (('to', 'have', 'a'), 8), (('i', 'wanted', 'to'), 8), (('i', 'was', 'a'), 8), (('it', 'was', 'a'), 8), (('he', 'would', 'have'), 8), (('would', 'be', 'a'), 8), (('this', 'is', 'a'), 8), (('one', 'of', 'those'), 8), (('to', 'see', 'the'), 8), (('it', 's', 'a'), 8), (('in', 'the', 'us'), 7), (('but', 'i', 'think'), 7), (('is', 'not', 'a'), 7)]\n",
      "Most common 3-grams of posts w score >100:\t[(('3', '3', '3'), 76), (('one', 'of', 'the'), 70), (('a', 'lot', 'of'), 46), (('at', 'the', 'time'), 34), (('out', 'of', 'the'), 28), (('the', 'rest', 'of'), 22), (('to', 'be', 'a'), 20), (('when', 'i', 'was'), 19), (('it', 'was', 'a'), 19), (('there', 'was', 'a'), 18), (('in', 'the', 'world'), 17), (('i', 'used', 'to'), 17), (('this', 'is', 'the'), 17), (('one', 'of', 'my'), 17), (('in', 'front', 'of'), 17), (('a', 'bunch', 'of'), 15), (('this', 'is', 'a'), 15), (('this', 'is', 'why'), 15), (('would', 'have', 'been'), 15), (('of', 'the', 'most'), 14), (('in', 'the', 'us'), 14), (('to', 'be', 'the'), 14), (('of', 'the', 'best'), 14), (('i', 'have', 'to'), 13), (('reminds', 'me', 'of'), 13), (('the', 'fact', 'that'), 13), (('he', 'was', 'a'), 13), (('be', 'able', 'to'), 13), (('i', 'feel', 'like'), 12), (('i', 'had', 'a'), 12), (('a', 'few', 'years'), 12), (('in', 'the', 'middle'), 12), (('i', 'can', 't'), 12), (('some', 'of', 'the'), 12), (('was', 'going', 'to'), 12), (('part', 'of', 'the'), 11), (('if', 'you', 'are'), 11), (('rest', 'of', 'the'), 11), (('all', 'of', 'the'), 11), (('you', 'have', 'to'), 11)]\n",
      "Most common 3-grams of posts w score >10:\t[(('one', 'of', 'the'), 97), (('a', 'lot', 'of'), 90), (('when', 'i', 'was'), 57), (('it', 'was', 'a'), 45), (('at', 'the', 'time'), 35), (('reminds', 'me', 'of'), 34), (('i', 'don', 't'), 30), (('some', 'of', 'the'), 28), (('he', 'was', 'a'), 27), (('the', 'rest', 'of'), 27), (('to', 'be', 'a'), 26), (('this', 'is', 'a'), 26), (('i', 'went', 'to'), 26), (('is', 'one', 'of'), 26), (('most', 'of', 'the'), 25), (('was', 'one', 'of'), 25), (('i', 'was', 'in'), 25), (('3', '3', '3'), 25), (('there', 'was', 'a'), 24), (('in', 'the', 'us'), 24), (('one', 'of', 'my'), 24), (('i', 'had', 'a'), 23), (('part', 'of', 'the'), 22), (('the', 'same', 'thing'), 22), (('i', 'have', 'a'), 22), (('i', 'was', 'a'), 22), (('i', 'used', 'to'), 21), (('out', 'of', 'the'), 20), (('be', 'able', 'to'), 20), (('the', 'fact', 'that'), 20), (('one', 'of', 'those'), 19), (('you', 'have', 'to'), 19), (('i', 'm', 'not'), 19), (('it', 'was', 'the'), 19), (('the', 'first', 'time'), 19), (('you', 'want', 'to'), 18), (('this', 'is', 'why'), 18), (('all', 'the', 'time'), 18), (('in', 'the', 'world'), 18), (('the', 'end', 'of'), 17)]\n",
      "Most common 3-grams of posts w score >1:\t[(('one', 'of', 'the'), 153), (('a', 'lot', 'of'), 134), (('when', 'i', 'was'), 75), (('this', 'is', 'the'), 56), (('it', 'was', 'a'), 51), (('reminds', 'me', 'of'), 50), (('i', 'was', 'in'), 49), (('to', 'be', 'a'), 45), (('there', 'was', 'a'), 40), (('i', 'don', 't'), 40), (('in', 'the', 'world'), 40), (('one', 'of', 'my'), 40), (('this', 'is', 'why'), 40), (('the', 'rest', 'of'), 40), (('out', 'of', 'the'), 38), (('this', 'is', 'a'), 38), (('in', 'the', 'us'), 37), (('he', 'was', 'a'), 36), (('i', 'was', 'a'), 36), (('a', 'few', 'years'), 35), (('i', 'feel', 'like'), 35), (('i', 'want', 'to'), 35), (('there', 'is', 'a'), 34), (('is', 'one', 'of'), 34), (('at', 'the', 'time'), 32), (('was', 'one', 'of'), 32), (('i', 'have', 'a'), 32), (('you', 'have', 'to'), 32), (('the', 'fact', 'that'), 31), (('it', 's', 'a'), 31), (('one', 'of', 'those'), 30), (('i', 'used', 'to'), 30), (('of', 'the', 'best'), 28), (('be', 'able', 'to'), 28), (('a', 'bunch', 'of'), 27), (('would', 'have', 'been'), 27), (('all', 'the', 'time'), 27), (('at', 'the', 'end'), 27), (('thought', 'it', 'was'), 27), (('i', 'had', 'to'), 26)]\n",
      "\n",
      "Most common 4-grams of all posts:\t[(('3', '3', '3', '3'), 98), (('was', 'one', 'of', 'the'), 51), (('one', 'of', 'the', 'best'), 51), (('a', 'lot', 'of', 'people'), 49), (('the', 'rest', 'of', 'the'), 49), (('one', 'of', 'the', 'most'), 46), (('is', 'one', 'of', 'the'), 46), (('a', 'few', 'years', 'ago'), 42), (('when', 'i', 'was', 'a'), 41), (('the', 'end', 'of', 'the'), 40), (('one', 'of', 'my', 'favorite'), 37), (('in', 'the', 'middle', 'of'), 37), (('this', 'reminds', 'me', 'of'), 36), (('at', 'the', 'end', 'of'), 35), (('reminds', 'me', 'of', 'the'), 35), (('i', 'was', 'a', 'kid'), 34), (('my', 'wife', 'and', 'i'), 29), (('when', 'i', 'was', 'in'), 29), (('at', 'the', 'same', 'time'), 28), (('when', 'it', 'comes', 'to'), 26), (('for', 'the', 'rest', 'of'), 25), (('i', 'thought', 'it', 'was'), 24), (('to', 'be', 'able', 'to'), 23), (('one', 'of', 'the', 'greatest'), 23), (('this', 'is', 'why', 'i'), 22), (('if', 'you', 'want', 'to'), 22), (('in', 'the', 'first', 'place'), 21), (('this', 'is', 'one', 'of'), 21), (('i', 'would', 'like', 'to'), 20), (('a', 'lot', 'of', 'money'), 20), (('back', 'in', 'the', 'day'), 19), (('top', 'post', 'on', 'reddit'), 19), (('with', 'all', 'the', 'other'), 19), (('for', 'the', 'first', 'time'), 19), (('i', 'had', 'no', 'idea'), 18), (('hey', 'this', 'is', 'now'), 18), (('this', 'is', 'now', 'the'), 18), (('is', 'now', 'the', 'top'), 18), (('now', 'the', 'top', 'post'), 18), (('the', 'top', 'post', 'on'), 18)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common 4-grams of posts w score >10,000:\t[(('one', 'of', 'the', 'most'), 6), (('thanks', 'for', 'the', 'gold'), 4), (('a', 'lot', 'of', 'people'), 3), (('the', 'rest', 'of', 'my'), 3), (('rest', 'of', 'my', 'life'), 3), (('when', 'i', 'was', 'a'), 3), (('i', 'do', 'know', 'what'), 3), (('another', 'brick', 'in', 'the'), 3), (('brick', 'in', 'the', 'wall'), 3), (('in', 'the', 'wall', 'part'), 3), (('thank', 'you', 'for', 'the'), 3), (('a', 'lot', 'of', 'money'), 3), (('one', 'of', 'the', 'greatest'), 3), (('probably', 'one', 'of', 'the'), 3), (('colonel', 'robert', 'gould', 'shaw'), 2), (('was', 'killed', 'during', 'the'), 2), (('his', 'brave', 'and', 'devoted'), 2), (('to', 'hang', 'out', 'with'), 2), (('in', 'the', 'title', 'but'), 2), (('9', 'minutes', 'a', 'day'), 2), (('for', 'a', 'lot', 'of'), 2), (('it', 'turns', 'out', 'the'), 2), (('the', 'back', 'of', 'the'), 2), (('back', 'of', 'the', 'room'), 2), (('she', 'was', 'in', 'her'), 2), (('is', 'a', 'link', 'to'), 2), (('in', 'the', 'us', 'if'), 2), (('the', 'us', 'if', 'you'), 2), (('us', 'if', 'you', 'are'), 2), (('if', 'you', 'are', 'a'), 2), (('you', 'are', 'a', 'living'), 2), (('are', 'a', 'living', 'kidney'), 2), (('a', 'living', 'kidney', 'donor'), 2), (('you', 'freely', 'donate', 'a'), 2), (('freely', 'donate', 'a', 'kidney'), 2), (('donate', 'a', 'kidney', 'while'), 2), (('a', 'kidney', 'while', 'still'), 2), (('kidney', 'while', 'still', 'alive'), 2), (('while', 'still', 'alive', 'you'), 2), (('still', 'alive', 'you', 'are'), 2)]\n",
      "Most common 4-grams of posts w score >1,000:\t[(('was', 'one', 'of', 'the'), 6), (('the', 'rest', 'of', 'the'), 5), (('a', 'few', 'years', 'ago'), 5), (('a', 'lot', 'of', 'people'), 5), (('in', 'the', 'middle', 'of'), 5), (('one', 'of', 'the', 'best'), 5), (('on', 'the', 'other', 'hand'), 4), (('one', 'of', 'my', 'favorite'), 4), (('when', 'i', 'was', 'a'), 4), (('i', 'was', 'a', 'kid'), 4), (('the', 'fact', 'that', 'he'), 4), (('when', 'it', 'comes', 'to'), 4), (('reminds', 'me', 'of', 'the'), 4), (('always', 'in', 'a', 'bad'), 4), (('in', 'a', 'bad', 'mood'), 4), (('one', 'of', 'the', 'most'), 4), (('when', 'i', 'was', 'in'), 4), (('this', 'reminds', 'me', 'of'), 4), (('very', 'very', 'very', 'very'), 4), (('the', 'top', 'of', 'the'), 3), (('i', 'do', 'want', 'to'), 3), (('you', 'have', 'to', 'be'), 3), (('got', 'a', 'lot', 'of'), 3), (('at', 'the', 'age', 'of'), 3), (('i', 'would', 'like', 'to'), 3), (('here', 'https', 'is', 'the'), 3), (('back', 'in', 'the', 'day'), 3), (('he', 'was', 'the', 'guy'), 3), (('was', 'a', 'lot', 'of'), 3), (('as', 'part', 'of', 'a'), 3), (('thank', 'you', 'for', 'the'), 3), (('one', 'of', 'the', 'greatest'), 3), (('i', 'had', 'no', 'idea'), 3), (('if', 'you', 'watch', 'the'), 3), (('at', 'the', 'end', 'of'), 3), (('the', 'end', 'of', 'the'), 3), (('i', 'used', 'to', 'work'), 3), (('want', 'to', 'take', 'the'), 3), (('was', 'always', 'in', 'a'), 3), (('i', 'felt', 'like', 'i'), 3)]\n",
      "Most common 4-grams of posts w score >100:\t[(('3', '3', '3', '3'), 75), (('one', 'of', 'the', 'best'), 13), (('the', 'rest', 'of', 'the'), 11), (('one', 'of', 'the', 'most'), 10), (('in', 'the', 'middle', 'of'), 9), (('one', 'of', 'my', 'favorite'), 8), (('a', 'lot', 'of', 'people'), 7), (('the', 'end', 'of', 'the'), 7), (('i', 'would', 'love', 'to'), 7), (('at', 'the', 'same', 'time'), 7), (('is', 'one', 'of', 'the'), 7), (('a', 'few', 'years', 'ago'), 6), (('was', 'one', 'of', 'the'), 6), (('i', 'used', 'to', 'work'), 6), (('this', 'reminds', 'me', 'of'), 6), (('it', 'would', 'have', 'been'), 6), (('at', 'the', 'time', 'and'), 5), (('this', 'is', 'why', 'i'), 5), (('in', 'the', 'first', 'place'), 5), (('i', 'm', 'not', 'sure'), 5), (('did', 'the', 'same', 'thing'), 5), (('the', 'middle', 'of', 'the'), 5), (('if', 'i', 'remember', 'correctly'), 5), (('to', 'be', 'able', 'to'), 5), (('the', 'fuck', 'out', 'of'), 4), (('reminds', 'me', 'of', 'when'), 4), (('if', 'he', 'wanted', 'to'), 4), (('the', 'fact', 'that', 'the'), 4), (('i', 'would', 'like', 'to'), 4), (('i', 'do', 'think', 'that'), 4), (('the', 'top', 'of', 'the'), 4), (('as', 'a', 'side', 'note'), 4), (('it', 'was', 'one', 'of'), 4), (('would', 'love', 'to', 'see'), 4), (('i', 'thought', 'it', 'was'), 4), (('a', 'witch', 'a', 'witch'), 4), (('reminds', 'me', 'of', 'the'), 4), (('when', 'i', 'was', 'a'), 4), (('in', 'one', 'of', 'the'), 4), (('at', 'the', 'end', 'of'), 4)]\n",
      "Most common 4-grams of posts w score >10:\t[(('3', '3', '3', '3'), 23), (('was', 'one', 'of', 'the'), 18), (('a', 'lot', 'of', 'people'), 17), (('is', 'one', 'of', 'the'), 14), (('the', 'rest', 'of', 'the'), 13), (('this', 'reminds', 'me', 'of'), 12), (('in', 'the', 'middle', 'of'), 12), (('when', 'it', 'comes', 'to'), 12), (('when', 'i', 'was', 'a'), 12), (('for', 'the', 'rest', 'of'), 11), (('the', 'end', 'of', 'the'), 11), (('i', 'was', 'a', 'kid'), 11), (('at', 'the', 'end', 'of'), 10), (('at', 'the', 'same', 'time'), 10), (('one', 'of', 'the', 'best'), 9), (('reminds', 'me', 'of', 'a'), 9), (('one', 'of', 'my', 'favorite'), 9), (('a', 'few', 'years', 'ago'), 9), (('i', 'don', 't', 'know'), 8), (('if', 'you', 'want', 'to'), 8), (('the', 'first', 'time', 'i'), 8), (('when', 'i', 'was', 'in'), 8), (('reminds', 'me', 'of', 'the'), 8), (('for', 'the', 'first', 'time'), 8), (('in', 'the', 'first', 'place'), 7), (('did', 'the', 'same', 'thing'), 7), (('most', 'of', 'the', 'time'), 7), (('to', 'be', 'able', 'to'), 7), (('this', 'is', 'one', 'of'), 7), (('this', 'is', 'why', 'i'), 7), (('a', 'piece', 'of', 'shit'), 7), (('one', 'of', 'the', 'most'), 7), (('prince', 'of', 'egypt', 'was'), 7), (('i', 'was', 'going', 'to'), 6), (('back', 'in', 'the', 'day'), 6), (('and', 'a', 'lot', 'of'), 6), (('my', 'wife', 'and', 'i'), 6), (('i', 'would', 'like', 'to'), 6), (('i', 'thought', 'it', 'was'), 6), (('the', 'middle', 'of', 'the'), 6)]\n",
      "Most common 4-grams of posts w score >1:\t[(('one', 'of', 'the', 'best'), 23), (('is', 'one', 'of', 'the'), 21), (('a', 'few', 'years', 'ago'), 20), (('the', 'rest', 'of', 'the'), 20), (('was', 'one', 'of', 'the'), 20), (('reminds', 'me', 'of', 'the'), 19), (('one', 'of', 'the', 'most'), 19), (('the', 'end', 'of', 'the'), 18), (('when', 'i', 'was', 'a'), 18), (('a', 'lot', 'of', 'people'), 17), (('top', 'post', 'on', 'reddit'), 17), (('with', 'all', 'the', 'other'), 17), (('at', 'the', 'end', 'of'), 17), (('my', 'wife', 'and', 'i'), 17), (('hey', 'this', 'is', 'now'), 16), (('this', 'is', 'now', 'the'), 16), (('is', 'now', 'the', 'top'), 16), (('now', 'the', 'top', 'post'), 16), (('the', 'top', 'post', 'on'), 16), (('post', 'on', 'reddit', 'it'), 16), (('on', 'reddit', 'it', 'will'), 16), (('reddit', 'it', 'will', 'be'), 16), (('it', 'will', 'be', 'recorded'), 16), (('will', 'be', 'recorded', 'at'), 16), (('be', 'recorded', 'at', 'with'), 16), (('recorded', 'at', 'with', 'all'), 16), (('at', 'with', 'all', 'the'), 16), (('all', 'the', 'other', 'top'), 16), (('the', 'other', 'top', 'posts'), 16), (('one', 'of', 'my', 'favorite'), 16), (('i', 'was', 'a', 'kid'), 15), (('when', 'i', 'was', 'in'), 15), (('this', 'reminds', 'me', 'of'), 13), (('i', 'thought', 'it', 'was'), 13), (('i', 'was', 'in', 'the'), 12), (('one', 'of', 'the', 'greatest'), 12), (('in', 'the', 'middle', 'of'), 11), (('that', 'a', 'lot', 'of'), 10), (('if', 'you', 'want', 'to'), 10), (('to', 'be', 'able', 'to'), 9)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "common_ngram_summary(\"todayilearned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "oriental-bankruptcy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on post 1/300 in askreddit\n",
      "Working on post 2/300 in askreddit\n",
      "Working on post 3/300 in askreddit\n",
      "Working on post 4/300 in askreddit\n",
      "Working on post 5/300 in askreddit\n",
      "Working on post 6/300 in askreddit\n",
      "Working on post 7/300 in askreddit\n",
      "Working on post 8/300 in askreddit\n",
      "Working on post 9/300 in askreddit\n",
      "Working on post 10/300 in askreddit\n",
      "Working on post 11/300 in askreddit\n",
      "Working on post 12/300 in askreddit\n",
      "Working on post 13/300 in askreddit\n",
      "Working on post 14/300 in askreddit\n",
      "Working on post 15/300 in askreddit\n",
      "Working on post 16/300 in askreddit\n",
      "Working on post 17/300 in askreddit\n",
      "Working on post 18/300 in askreddit\n",
      "Working on post 19/300 in askreddit\n",
      "Working on post 20/300 in askreddit\n",
      "Working on post 21/300 in askreddit\n",
      "Working on post 22/300 in askreddit\n",
      "Working on post 23/300 in askreddit\n",
      "Working on post 24/300 in askreddit\n",
      "Working on post 25/300 in askreddit\n",
      "Working on post 26/300 in askreddit\n",
      "Working on post 27/300 in askreddit\n",
      "Working on post 28/300 in askreddit\n",
      "Working on post 29/300 in askreddit\n",
      "Working on post 30/300 in askreddit\n",
      "Working on post 31/300 in askreddit\n",
      "Working on post 32/300 in askreddit\n",
      "Working on post 33/300 in askreddit\n",
      "Working on post 34/300 in askreddit\n",
      "Working on post 35/300 in askreddit\n",
      "Working on post 36/300 in askreddit\n",
      "Working on post 37/300 in askreddit\n",
      "Working on post 38/300 in askreddit\n",
      "Working on post 39/300 in askreddit\n",
      "Working on post 40/300 in askreddit\n",
      "Working on post 41/300 in askreddit\n",
      "Working on post 42/300 in askreddit\n",
      "Working on post 43/300 in askreddit\n",
      "Working on post 44/300 in askreddit\n",
      "Working on post 45/300 in askreddit\n",
      "Working on post 46/300 in askreddit\n",
      "Working on post 47/300 in askreddit\n",
      "Working on post 48/300 in askreddit\n",
      "Working on post 49/300 in askreddit\n",
      "Working on post 50/300 in askreddit\n",
      "Working on post 51/300 in askreddit\n",
      "Working on post 52/300 in askreddit\n",
      "Working on post 53/300 in askreddit\n",
      "Working on post 54/300 in askreddit\n",
      "Working on post 55/300 in askreddit\n",
      "Working on post 56/300 in askreddit\n",
      "Working on post 57/300 in askreddit\n",
      "Working on post 58/300 in askreddit\n",
      "Working on post 59/300 in askreddit\n",
      "Working on post 60/300 in askreddit\n",
      "Working on post 61/300 in askreddit\n",
      "Working on post 62/300 in askreddit\n",
      "Working on post 63/300 in askreddit\n",
      "Working on post 64/300 in askreddit\n",
      "Working on post 65/300 in askreddit\n",
      "Working on post 66/300 in askreddit\n",
      "Working on post 67/300 in askreddit\n",
      "Working on post 68/300 in askreddit\n",
      "Working on post 69/300 in askreddit\n",
      "Working on post 70/300 in askreddit\n",
      "Working on post 71/300 in askreddit\n",
      "Working on post 72/300 in askreddit\n",
      "Working on post 73/300 in askreddit\n",
      "Working on post 74/300 in askreddit\n",
      "Working on post 75/300 in askreddit\n",
      "Working on post 76/300 in askreddit\n",
      "Working on post 77/300 in askreddit\n",
      "Working on post 78/300 in askreddit\n",
      "Working on post 79/300 in askreddit\n",
      "Working on post 80/300 in askreddit\n",
      "Working on post 81/300 in askreddit\n",
      "Working on post 82/300 in askreddit\n",
      "Working on post 83/300 in askreddit\n",
      "Working on post 84/300 in askreddit\n",
      "Working on post 85/300 in askreddit\n",
      "Working on post 86/300 in askreddit\n",
      "Working on post 87/300 in askreddit\n",
      "Working on post 88/300 in askreddit\n",
      "Working on post 89/300 in askreddit\n",
      "Working on post 90/300 in askreddit\n",
      "Working on post 91/300 in askreddit\n",
      "Working on post 92/300 in askreddit\n",
      "Working on post 93/300 in askreddit\n",
      "Working on post 94/300 in askreddit\n",
      "Working on post 95/300 in askreddit\n",
      "Working on post 96/300 in askreddit\n",
      "Working on post 97/300 in askreddit\n",
      "Working on post 98/300 in askreddit\n",
      "Working on post 99/300 in askreddit\n",
      "Working on post 100/300 in askreddit\n",
      "Working on post 101/300 in askreddit\n",
      "Working on post 102/300 in askreddit\n",
      "Working on post 103/300 in askreddit\n",
      "Working on post 104/300 in askreddit\n",
      "Working on post 105/300 in askreddit\n",
      "Working on post 106/300 in askreddit\n",
      "Working on post 107/300 in askreddit\n",
      "Working on post 108/300 in askreddit\n",
      "Working on post 109/300 in askreddit\n",
      "Working on post 110/300 in askreddit\n",
      "Working on post 111/300 in askreddit\n",
      "Working on post 112/300 in askreddit\n",
      "Working on post 113/300 in askreddit\n",
      "Working on post 114/300 in askreddit\n",
      "Working on post 115/300 in askreddit\n",
      "Working on post 116/300 in askreddit\n",
      "Working on post 117/300 in askreddit\n",
      "Working on post 118/300 in askreddit\n",
      "Working on post 119/300 in askreddit\n",
      "Working on post 120/300 in askreddit\n",
      "Working on post 121/300 in askreddit\n",
      "Working on post 122/300 in askreddit\n",
      "Working on post 123/300 in askreddit\n",
      "Working on post 124/300 in askreddit\n",
      "Working on post 125/300 in askreddit\n",
      "Working on post 126/300 in askreddit\n",
      "Working on post 127/300 in askreddit\n",
      "Working on post 128/300 in askreddit\n",
      "Working on post 129/300 in askreddit\n",
      "Working on post 130/300 in askreddit\n",
      "Working on post 131/300 in askreddit\n",
      "Working on post 132/300 in askreddit\n",
      "Working on post 133/300 in askreddit\n",
      "Working on post 134/300 in askreddit\n",
      "Working on post 135/300 in askreddit\n",
      "Working on post 136/300 in askreddit\n",
      "Working on post 137/300 in askreddit\n",
      "Working on post 138/300 in askreddit\n",
      "Working on post 139/300 in askreddit\n",
      "Working on post 140/300 in askreddit\n",
      "Working on post 141/300 in askreddit\n",
      "Working on post 142/300 in askreddit\n",
      "Working on post 143/300 in askreddit\n",
      "Working on post 144/300 in askreddit\n",
      "Working on post 145/300 in askreddit\n",
      "Working on post 146/300 in askreddit\n",
      "Working on post 147/300 in askreddit\n",
      "Working on post 148/300 in askreddit\n",
      "Working on post 149/300 in askreddit\n",
      "Working on post 150/300 in askreddit\n",
      "Working on post 151/300 in askreddit\n",
      "Working on post 152/300 in askreddit\n",
      "Working on post 153/300 in askreddit\n",
      "Working on post 154/300 in askreddit\n",
      "Working on post 155/300 in askreddit\n",
      "Working on post 156/300 in askreddit\n",
      "Working on post 157/300 in askreddit\n",
      "Working on post 158/300 in askreddit\n",
      "Working on post 159/300 in askreddit\n",
      "Working on post 160/300 in askreddit\n",
      "Working on post 161/300 in askreddit\n",
      "Working on post 162/300 in askreddit\n",
      "Working on post 163/300 in askreddit\n",
      "Working on post 164/300 in askreddit\n",
      "Working on post 165/300 in askreddit\n",
      "Working on post 166/300 in askreddit\n",
      "Working on post 167/300 in askreddit\n",
      "Working on post 168/300 in askreddit\n",
      "Working on post 169/300 in askreddit\n",
      "Working on post 170/300 in askreddit\n",
      "Working on post 171/300 in askreddit\n",
      "Working on post 172/300 in askreddit\n",
      "Working on post 173/300 in askreddit\n",
      "Working on post 174/300 in askreddit\n",
      "Working on post 175/300 in askreddit\n",
      "Working on post 176/300 in askreddit\n",
      "Working on post 177/300 in askreddit\n",
      "Working on post 178/300 in askreddit\n",
      "Working on post 179/300 in askreddit\n",
      "Working on post 180/300 in askreddit\n",
      "Working on post 181/300 in askreddit\n",
      "Working on post 182/300 in askreddit\n",
      "Working on post 183/300 in askreddit\n",
      "Working on post 184/300 in askreddit\n",
      "Working on post 185/300 in askreddit\n",
      "Working on post 186/300 in askreddit\n",
      "Working on post 187/300 in askreddit\n",
      "Working on post 188/300 in askreddit\n",
      "Working on post 189/300 in askreddit\n",
      "Working on post 190/300 in askreddit\n",
      "Working on post 191/300 in askreddit\n",
      "Working on post 192/300 in askreddit\n",
      "Working on post 193/300 in askreddit\n",
      "Working on post 194/300 in askreddit\n",
      "Working on post 195/300 in askreddit\n",
      "Working on post 196/300 in askreddit\n",
      "Working on post 197/300 in askreddit\n",
      "Working on post 198/300 in askreddit\n",
      "Working on post 199/300 in askreddit\n",
      "Working on post 200/300 in askreddit\n",
      "Working on post 201/300 in askreddit\n",
      "Working on post 202/300 in askreddit\n",
      "Working on post 203/300 in askreddit\n",
      "Working on post 204/300 in askreddit\n",
      "Working on post 205/300 in askreddit\n",
      "Working on post 206/300 in askreddit\n",
      "Working on post 207/300 in askreddit\n",
      "Working on post 208/300 in askreddit\n",
      "Working on post 209/300 in askreddit\n",
      "Working on post 210/300 in askreddit\n",
      "Working on post 211/300 in askreddit\n",
      "Working on post 212/300 in askreddit\n",
      "Working on post 213/300 in askreddit\n",
      "Working on post 214/300 in askreddit\n",
      "Working on post 215/300 in askreddit\n",
      "Working on post 216/300 in askreddit\n",
      "Working on post 217/300 in askreddit\n",
      "Working on post 218/300 in askreddit\n",
      "Working on post 219/300 in askreddit\n",
      "Working on post 220/300 in askreddit\n",
      "Working on post 221/300 in askreddit\n",
      "Working on post 222/300 in askreddit\n",
      "Working on post 223/300 in askreddit\n",
      "Working on post 224/300 in askreddit\n",
      "Working on post 225/300 in askreddit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on post 226/300 in askreddit\n",
      "Working on post 227/300 in askreddit\n",
      "Working on post 228/300 in askreddit\n",
      "Working on post 229/300 in askreddit\n",
      "Working on post 230/300 in askreddit\n",
      "Working on post 231/300 in askreddit\n",
      "Working on post 232/300 in askreddit\n",
      "Working on post 233/300 in askreddit\n",
      "Working on post 234/300 in askreddit\n",
      "Working on post 235/300 in askreddit\n",
      "Working on post 236/300 in askreddit\n",
      "Working on post 237/300 in askreddit\n",
      "Working on post 238/300 in askreddit\n",
      "Working on post 239/300 in askreddit\n",
      "Working on post 240/300 in askreddit\n",
      "Working on post 241/300 in askreddit\n",
      "Working on post 242/300 in askreddit\n",
      "Working on post 243/300 in askreddit\n",
      "Working on post 244/300 in askreddit\n",
      "Working on post 245/300 in askreddit\n",
      "Working on post 246/300 in askreddit\n",
      "Working on post 247/300 in askreddit\n",
      "Working on post 248/300 in askreddit\n",
      "Working on post 249/300 in askreddit\n",
      "Working on post 250/300 in askreddit\n",
      "Working on post 251/300 in askreddit\n",
      "Working on post 252/300 in askreddit\n",
      "Working on post 253/300 in askreddit\n",
      "Working on post 254/300 in askreddit\n",
      "Working on post 255/300 in askreddit\n",
      "Working on post 256/300 in askreddit\n",
      "Working on post 257/300 in askreddit\n",
      "Working on post 258/300 in askreddit\n",
      "Working on post 259/300 in askreddit\n",
      "Working on post 260/300 in askreddit\n",
      "Working on post 261/300 in askreddit\n",
      "Working on post 262/300 in askreddit\n",
      "Working on post 263/300 in askreddit\n",
      "Working on post 264/300 in askreddit\n",
      "Working on post 265/300 in askreddit\n",
      "Working on post 266/300 in askreddit\n",
      "Working on post 267/300 in askreddit\n",
      "Working on post 268/300 in askreddit\n",
      "Working on post 269/300 in askreddit\n",
      "Working on post 270/300 in askreddit\n",
      "Working on post 271/300 in askreddit\n",
      "Working on post 272/300 in askreddit\n",
      "Working on post 273/300 in askreddit\n",
      "Working on post 274/300 in askreddit\n",
      "Working on post 275/300 in askreddit\n",
      "Working on post 276/300 in askreddit\n",
      "Working on post 277/300 in askreddit\n",
      "Working on post 278/300 in askreddit\n",
      "Working on post 279/300 in askreddit\n",
      "Working on post 280/300 in askreddit\n",
      "Working on post 281/300 in askreddit\n",
      "Working on post 282/300 in askreddit\n",
      "Working on post 283/300 in askreddit\n",
      "Working on post 284/300 in askreddit\n",
      "Working on post 285/300 in askreddit\n",
      "Working on post 286/300 in askreddit\n",
      "Working on post 287/300 in askreddit\n",
      "Working on post 288/300 in askreddit\n",
      "Working on post 289/300 in askreddit\n",
      "Working on post 290/300 in askreddit\n",
      "Working on post 291/300 in askreddit\n",
      "Working on post 292/300 in askreddit\n",
      "Working on post 293/300 in askreddit\n",
      "Working on post 294/300 in askreddit\n",
      "Working on post 295/300 in askreddit\n",
      "Working on post 296/300 in askreddit\n",
      "Working on post 297/300 in askreddit\n",
      "Working on post 298/300 in askreddit\n",
      "Working on post 299/300 in askreddit\n",
      "Working on post 300/300 in askreddit\n"
     ]
    }
   ],
   "source": [
    "build_subreddit_corpus(\"askreddit\", reddit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-dodge",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
